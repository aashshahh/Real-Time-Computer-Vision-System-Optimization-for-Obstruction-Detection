{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d756e3e3",
   "metadata": {},
   "source": [
    "ResNet Ensemble Working File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d879aaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'clear': 0, 'obstructed': 1}\n",
      "Train size: 14000 Val size: 3000 Test size: 3000\n"
     ]
    }
   ],
   "source": [
    "# === Data / Dataloaders for Ensemble Notebook ===\n",
    "\n",
    "import os, csv\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---- Paths ----\n",
    "CSV_DIR    = Path(r\"C:\\Users\\Andre\\Documents\\Machine Learning Project\\processed_csvs\")\n",
    "IMAGE_ROOT = CSV_DIR  # images live under this root\n",
    "\n",
    "# ---- Label mapping (must match training) ----\n",
    "LABEL_TO_IDX = {\"clear\": 0, \"obstructed\": 1}\n",
    "\n",
    "# ---- Normalization (ImageNet) ----\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def get_transforms():\n",
    "    \"\"\"Baseline transforms (no RandAugment) – same as your baseline models.\"\"\"\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    return train_tf, eval_tf\n",
    "\n",
    "train_tf, eval_tf = get_transforms()\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_path, transform, image_root=None):\n",
    "        self.rows = []\n",
    "        self.transform = transform\n",
    "        self.image_root = image_root\n",
    "\n",
    "        with open(csv_path, \"r\", newline=\"\") as f:\n",
    "            r = csv.DictReader(f)\n",
    "            assert {\"filename\",\"label\"}.issubset(r.fieldnames), f\"Missing headers in {csv_path}\"\n",
    "            for row in r:\n",
    "                fp = row[\"filename\"].strip()\n",
    "                if image_root is not None and not os.path.isabs(fp):\n",
    "                    fp = str(Path(image_root) / fp)\n",
    "                lab = row[\"label\"].strip().lower()\n",
    "                assert lab in LABEL_TO_IDX, f\"Unknown label {lab} in {csv_path}\"\n",
    "                self.rows.append((fp, LABEL_TO_IDX[lab]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, y = self.rows[idx]\n",
    "        with Image.open(img_path) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "        x = self.transform(im)\n",
    "        return x, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def make_loaders(batch_size=64, num_workers=0, pin_memory=False):\n",
    "    \"\"\"Create train/val/test loaders from train.csv, val.csv, test.csv.\"\"\"\n",
    "    train_csv = CSV_DIR / \"train.csv\"\n",
    "    val_csv   = CSV_DIR / \"val.csv\"\n",
    "    test_csv  = CSV_DIR / \"test.csv\"\n",
    "\n",
    "    train_ds = CSVDataset(train_csv, transform=train_tf, image_root=IMAGE_ROOT)\n",
    "    val_ds   = CSVDataset(val_csv,   transform=eval_tf,  image_root=IMAGE_ROOT)\n",
    "    test_ds  = CSVDataset(test_csv,  transform=eval_tf,  image_root=IMAGE_ROOT)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_ds, val_ds, test_ds\n",
    "\n",
    "# Instantiate loaders (we only really need test_loader here, but this is fine)\n",
    "device = torch.device(\"cpu\")  # keep everything CPU for ensemble\n",
    "train_loader, val_loader, test_loader, train_ds, val_ds, test_ds = make_loaders(\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"Label mapping:\", LABEL_TO_IDX)\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds), \"Test size:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b343a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_33680\\3003938082.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded baseline ResNet18 & ResNet50 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_33680\\3003938082.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# === Load Baseline ResNet18 & ResNet50 (FP32) ===\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# -------------------------------\n",
    "# Builder: Baseline ResNet18\n",
    "# -------------------------------\n",
    "def build_baseline_r18(weights_path: str):\n",
    "    model = models.resnet18(weights=None)\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, NUM_CLASSES)\n",
    "\n",
    "    state = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Builder: Baseline ResNet50\n",
    "# -------------------------------\n",
    "def build_baseline_r50(weights_path: str):\n",
    "    model = models.resnet50(weights=None)\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, NUM_CLASSES)\n",
    "\n",
    "    state = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Load both baselines\n",
    "# -------------------------------\n",
    "R18_BASE = \"resnet18_clear_obstructed_best.pt\"\n",
    "R50_BASE = \"resnet50_clear_obstructed_best.pt\"\n",
    "\n",
    "baseline_r18 = build_baseline_r18(R18_BASE)\n",
    "baseline_r50 = build_baseline_r50(R50_BASE)\n",
    "\n",
    "print(\"Loaded baseline ResNet18 & ResNet50 successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ec2533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\miniconda3\\envs\\cv-efficiency\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "x:\\miniconda3\\envs\\cv-efficiency\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "x:\\miniconda3\\envs\\cv-efficiency\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "x:\\miniconda3\\envs\\cv-efficiency\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FX-quantized ResNet18 & ResNet50 (via FX shell + state_dict) successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_33680\\779848892.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  quant_r18.load_state_dict(torch.load(R18_QUANT_SD, map_location=device))\n",
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_33680\\779848892.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  quant_r50.load_state_dict(torch.load(R50_QUANT_SD, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# === Build & Load FX-Quantized ResNet18 and ResNet50 (INT8) ===\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "try:\n",
    "    from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "except ImportError:\n",
    "    from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "R18_QUANT_SD = \"resnet18_clear_obstructed_quant_fx.pt\"   # state_dict you already saved\n",
    "R50_QUANT_SD = \"resnet50_clear_obstructed_quant_fx.pt\"   # state_dict you already saved\n",
    "\n",
    "\n",
    "def build_resnet18_quant_shell() -> nn.Module:\n",
    "    \"\"\"Rebuild quantized ResNet18 graph (no calibration), ready for loading quant SD.\"\"\"\n",
    "    # 1) Float model with 2-class head\n",
    "    m = models.resnet18(weights=None)\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_feats, NUM_CLASSES)\n",
    "    m.eval()\n",
    "\n",
    "    # 2) QConfig & FX prepare/convert\n",
    "    qconfig = get_default_qconfig(\"fbgemm\")\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    example_inputs = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "    prepared = prepare_fx(m, qconfig_mapping, example_inputs)\n",
    "    quantized = convert_fx(prepared)\n",
    "    quantized.eval()\n",
    "    return quantized\n",
    "\n",
    "\n",
    "def build_resnet50_quant_shell() -> nn.Module:\n",
    "    \"\"\"Rebuild quantized ResNet50 graph (no calibration), ready for loading quant SD.\"\"\"\n",
    "    m = models.resnet50(weights=None)\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_feats, NUM_CLASSES)\n",
    "    m.eval()\n",
    "\n",
    "    qconfig = get_default_qconfig(\"fbgemm\")\n",
    "    qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "    example_inputs = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "    prepared = prepare_fx(m, qconfig_mapping, example_inputs)\n",
    "    quantized = convert_fx(prepared)\n",
    "    quantized.eval()\n",
    "    return quantized\n",
    "\n",
    "\n",
    "# Build FX-quantized shells\n",
    "quant_r18 = build_resnet18_quant_shell()\n",
    "quant_r50 = build_resnet50_quant_shell()\n",
    "\n",
    "# Load the calibrated + converted weights into those shells\n",
    "quant_r18.load_state_dict(torch.load(R18_QUANT_SD, map_location=device))\n",
    "quant_r50.load_state_dict(torch.load(R50_QUANT_SD, map_location=device))\n",
    "\n",
    "quant_r18.to(device).eval()\n",
    "quant_r50.to(device).eval()\n",
    "\n",
    "print(\"Loaded FX-quantized ResNet18 & ResNet50 (via FX shell + state_dict) successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968b608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Ensemble (ResNet18 + ResNet50, FP32) ===\n",
      "Test accuracy: 0.9253\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[1600, 129], [95, 1176]]\n",
      "Per-class metrics:\n",
      "  clear (0):       precision 0.9440  recall 0.9254  f1 0.9346\n",
      "  obstructed (1):  precision 0.9011  recall 0.9253  f1 0.9130\n",
      "\n",
      "=== Quantized Ensemble (ResNet18 + ResNet50, FX INT8) ===\n",
      "Test accuracy: 0.9270\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[1606, 123], [96, 1175]]\n",
      "Per-class metrics:\n",
      "  clear (0):       precision 0.9436  recall 0.9289  f1 0.9362\n",
      "  obstructed (1):  precision 0.9052  recall 0.9245  f1 0.9148\n",
      "\n",
      "=== Full Ensemble (Baseline + Quantized ResNets) ===\n",
      "Test accuracy: 0.9257\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[1602, 127], [96, 1175]]\n",
      "Per-class metrics:\n",
      "  clear (0):       precision 0.9435  recall 0.9265  f1 0.9349\n",
      "  obstructed (1):  precision 0.9025  recall 0.9245  f1 0.9133\n"
     ]
    }
   ],
   "source": [
    "# === Ensemble Evaluation: ResNet18 + ResNet50 (Baseline / Quantized / Both) ===\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")   # keep everything on CPU for consistency with quant models\n",
    "\n",
    "\n",
    "def eval_ensemble(models, loader, name=\"Ensemble\"):\n",
    "    \"\"\"\n",
    "    Evaluate an ensemble of models on the test set.\n",
    "    - models: list of nn.Modules\n",
    "    - loader: DataLoader (test_loader)\n",
    "    Returns: (acc, cm, metrics_dict)\n",
    "    \"\"\"\n",
    "    for m in models:\n",
    "        m.to(device)\n",
    "        m.eval()\n",
    "\n",
    "    cm = torch.zeros(2, 2, dtype=torch.long)  # [[TN, FP],[FN, TP]]\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            # Sum logits from all models (mean and sum give same argmax)\n",
    "            logits_sum = None\n",
    "            for m in models:\n",
    "                out = m(xb)\n",
    "                logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "\n",
    "            logits = logits_sum / len(models)\n",
    "            pred = logits.argmax(1)\n",
    "\n",
    "            # update confusion matrix\n",
    "            for t, p in zip(yb.view(-1), pred.view(-1)):\n",
    "                cm[t.long(), p.long()] += 1\n",
    "\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total   += yb.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    TN, FP = cm[0,0].item(), cm[0,1].item()\n",
    "    FN, TP = cm[1,0].item(), cm[1,1].item()\n",
    "\n",
    "    def safe_div(a, b): \n",
    "        return a / b if b > 0 else 0.0\n",
    "\n",
    "    # Same \"clear\"/\"obstructed\" convention as everywhere else\n",
    "    prec_clear = safe_div(TN, TN + FN)\n",
    "    rec_clear  = safe_div(TN, TN + FP)\n",
    "    f1_clear   = safe_div(\n",
    "        2 * prec_clear * rec_clear,\n",
    "        prec_clear + rec_clear\n",
    "    ) if (prec_clear + rec_clear) > 0 else 0.0\n",
    "\n",
    "    prec_obst = safe_div(TP, TP + FP)\n",
    "    rec_obst  = safe_div(TP, TP + FN)\n",
    "    f1_obst   = safe_div(\n",
    "        2 * prec_obst * rec_obst,\n",
    "        prec_obst + rec_obst\n",
    "    ) if (prec_obst + rec_obst) > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Test accuracy:\", f\"{acc:.4f}\")\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(cm.tolist())\n",
    "    print(\"Per-class metrics:\")\n",
    "    print(f\"  clear (0):       precision {prec_clear:.4f}  recall {rec_clear:.4f}  f1 {f1_clear:.4f}\")\n",
    "    print(f\"  obstructed (1):  precision {prec_obst:.4f}  recall {rec_obst:.4f}  f1 {f1_obst:.4f}\")\n",
    "\n",
    "    return acc, cm, {\n",
    "        \"clear\":      (prec_clear, rec_clear, f1_clear),\n",
    "        \"obstructed\": (prec_obst, rec_obst, f1_obst),\n",
    "    }\n",
    "\n",
    "\n",
    "# 1) Baseline ensemble: ResNet18 + ResNet50 (FP32)\n",
    "ens_base_acc, ens_base_cm, ens_base_metrics = eval_ensemble(\n",
    "    [baseline_r18, baseline_r50],\n",
    "    test_loader,\n",
    "    name=\"Baseline Ensemble (ResNet18 + ResNet50, FP32)\"\n",
    ")\n",
    "\n",
    "# 2) Quantized ensemble: ResNet18 + ResNet50 (FX INT8)\n",
    "ens_quant_acc, ens_quant_cm, ens_quant_metrics = eval_ensemble(\n",
    "    [quant_r18, quant_r50],\n",
    "    test_loader,\n",
    "    name=\"Quantized Ensemble (ResNet18 + ResNet50, FX INT8)\"\n",
    ")\n",
    "\n",
    "# 3) Full ensemble: all four models together (optional but interesting)\n",
    "ens_all_acc, ens_all_cm, ens_all_metrics = eval_ensemble(\n",
    "    [baseline_r18, baseline_r50, quant_r18, quant_r50],\n",
    "    test_loader,\n",
    "    name=\"Full Ensemble (Baseline + Quantized ResNets)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e177ebea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline Ensemble (ResNet18 + ResNet50, FP32) (CPU) ===\n",
      "Single image:  mean 27.31 ms  p95 28.89 ms  FPS ~36.6\n",
      "Batch (64): mean 1360.57 ms  p95 1389.96 ms  Throughput ~47.0 img/s\n",
      "\n",
      "=== Quantized Ensemble (ResNet18 + ResNet50, FX INT8) (CPU) ===\n",
      "Single image:  mean 12.39 ms  p95 13.39 ms  FPS ~80.7\n",
      "Batch (64): mean 232.23 ms  p95 249.53 ms  Throughput ~275.6 img/s\n",
      "\n",
      "=== Full Ensemble (Baseline + Quantized ResNets) (CPU) ===\n",
      "Single image:  mean 40.75 ms  p95 42.50 ms  FPS ~24.5\n",
      "Batch (64): mean 1625.86 ms  p95 1730.03 ms  Throughput ~39.4 img/s\n"
     ]
    }
   ],
   "source": [
    "# === Latency & Throughput Benchmark for ResNet Ensembles (CPU) ===\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")  # keep CPU for fair comparison\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_ensemble_cpu(models, loader, warmup: int = 20, iters: int = 200, name: str = \"Ensemble\"):\n",
    "    \"\"\"\n",
    "    Benchmark an ensemble of models on CPU:\n",
    "      - single-image latency\n",
    "      - batch latency & throughput\n",
    "\n",
    "    Assumes:\n",
    "      - models: list of nn.Module\n",
    "      - loader: DataLoader (we'll grab a single batch from it)\n",
    "    \"\"\"\n",
    "    # Move all models to CPU + eval\n",
    "    models = [m.to(device).eval() for m in models]\n",
    "\n",
    "    # Grab one batch from the loader\n",
    "    xbB, _ = next(iter(loader))\n",
    "    xbB = xbB.to(device)      # [B, 3, 224, 224]\n",
    "    xb1 = xbB[:1]             # [1, 3, 224, 224]\n",
    "\n",
    "    times_single, times_batch = [], []\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        # single image\n",
    "        logits_sum = None\n",
    "        for m in models:\n",
    "            out = m(xb1)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "\n",
    "        # batch\n",
    "        logits_sum = None\n",
    "        for m in models:\n",
    "            out = m(xbB)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "\n",
    "    # Single-image timing\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        logits_sum = None\n",
    "        for m in models:\n",
    "            out = m(xb1)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "        times_single.append(time.perf_counter() - t0)\n",
    "\n",
    "    # Batch timing\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        logits_sum = None\n",
    "        for m in models:\n",
    "            out = m(xbB)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "        times_batch.append(time.perf_counter() - t0)\n",
    "\n",
    "    def stats(ts):\n",
    "        ts = np.array(ts) * 1000.0  # sec → ms\n",
    "        return ts.mean(), np.percentile(ts, 95)\n",
    "\n",
    "    m1, p951 = stats(times_single)\n",
    "    mB, p95B = stats(times_batch)\n",
    "    bsz = xbB.size(0)\n",
    "\n",
    "    fps_single = 1000.0 / m1\n",
    "    fps_batch  = (bsz * 1000.0) / mB\n",
    "\n",
    "    print(f\"\\n=== {name} (CPU) ===\")\n",
    "    print(f\"Single image:  mean {m1:.2f} ms  p95 {p951:.2f} ms  FPS ~{fps_single:.1f}\")\n",
    "    print(f\"Batch ({bsz}): mean {mB:.2f} ms  p95 {p95B:.2f} ms  Throughput ~{fps_batch:.1f} img/s\")\n",
    "\n",
    "    return {\n",
    "        \"single_mean_ms\": m1,\n",
    "        \"single_p95_ms\": p951,\n",
    "        \"single_fps\": fps_single,\n",
    "        \"batch_mean_ms\": mB,\n",
    "        \"batch_p95_ms\": p95B,\n",
    "        \"batch_fps\": fps_batch,\n",
    "        \"batch_size\": bsz,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---- Run benchmarks for the three ensembles ----\n",
    "\n",
    "ens_base_stats = benchmark_ensemble_cpu(\n",
    "    [baseline_r18, baseline_r50],\n",
    "    test_loader,\n",
    "    name=\"Baseline Ensemble (ResNet18 + ResNet50, FP32)\"\n",
    ")\n",
    "\n",
    "ens_quant_stats = benchmark_ensemble_cpu(\n",
    "    [quant_r18, quant_r50],\n",
    "    test_loader,\n",
    "    name=\"Quantized Ensemble (ResNet18 + ResNet50, FX INT8)\"\n",
    ")\n",
    "\n",
    "ens_all_stats = benchmark_ensemble_cpu(\n",
    "    [baseline_r18, baseline_r50, quant_r18, quant_r50],\n",
    "    test_loader,\n",
    "    name=\"Full Ensemble (Baseline + Quantized ResNets)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf835a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================================================\n",
    "# Hard-coded metrics from your logs (NO recomputation)\n",
    "# ======================================================\n",
    "\n",
    "labels = [\n",
    "    \"R18 FP32\",\n",
    "    \"R18 INT8\",\n",
    "    \"R50 FP32\",\n",
    "    \"R50 INT8\",\n",
    "    \"Ens FP32\",\n",
    "    \"Ens INT8\",\n",
    "    \"Ens Full\",\n",
    "]\n",
    "\n",
    "# ---- Accuracy ----\n",
    "acc = np.array([\n",
    "    0.9147,  # R18 baseline (CPU summary)\n",
    "    0.9150,  # R18 quant\n",
    "    0.9180,  # R50 baseline\n",
    "    0.9177,  # R50 quant\n",
    "    0.9253,  # Ensemble baseline\n",
    "    0.9270,  # Ensemble quant\n",
    "    0.9257,  # Ensemble full\n",
    "])\n",
    "\n",
    "# ---- Per-class recalls (from your text) ----\n",
    "# Clear (class 0) recall\n",
    "rec_clear = np.array([\n",
    "    0.9190,  # R18 base\n",
    "    0.9213,  # R18 quant\n",
    "    0.9161,  # R50 base\n",
    "    0.9202,  # R50 quant\n",
    "    0.9254,  # Ens base\n",
    "    0.9289,  # Ens quant\n",
    "    0.9265,  # Ens full\n",
    "])\n",
    "\n",
    "# Obstructed (class 1) recall\n",
    "rec_obst = np.array([\n",
    "    0.9087,  # R18 base\n",
    "    0.9064,  # R18 quant\n",
    "    0.9205,  # R50 base\n",
    "    0.9142,  # R50 quant\n",
    "    0.9253,  # Ens base\n",
    "    0.9245,  # Ens quant\n",
    "    0.9245,  # Ens full\n",
    "])\n",
    "\n",
    "# Obstructed F1\n",
    "f1_obst = np.array([\n",
    "    0.9002,  # R18 base\n",
    "    0.9004,  # R18 quant\n",
    "    0.9049,  # R50 base\n",
    "    0.9039,  # R50 quant\n",
    "    0.9130,  # Ens base\n",
    "    0.9148,  # Ens quant\n",
    "    0.9133,  # Ens full\n",
    "])\n",
    "\n",
    "# ---- Throughput (CPU) ----\n",
    "# Single-image FPS\n",
    "fps_single = np.array([\n",
    "    132.0,  # R18 base\n",
    "    289.1,  # R18 quant\n",
    "    50.1,   # R50 base\n",
    "    121.5,  # R50 quant\n",
    "    36.6,   # Ens base\n",
    "    80.7,   # Ens quant\n",
    "    24.5,   # Ens full\n",
    "])\n",
    "\n",
    "# Batch throughput (img/s)\n",
    "throughput_batch = np.array([\n",
    "    213.4,  # R18 base\n",
    "    1069.8, # R18 quant\n",
    "    58.8,   # R50 base\n",
    "    374.8,  # R50 quant\n",
    "    47.0,   # Ens base\n",
    "    275.6,  # Ens quant\n",
    "    39.4,   # Ens full\n",
    "])\n",
    "\n",
    "# ======================================================\n",
    "# Plot – 2×2 grid\n",
    "# ======================================================\n",
    "plt.style.use(\"default\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "fig.suptitle(\"ResNet18 / ResNet50 – Singles vs Ensembles (Metrics & CPU Throughput)\",\n",
    "             fontsize=16, weight=\"bold\")\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "bar_width = 0.6\n",
    "\n",
    "# Color mapping (just for visual grouping)\n",
    "colors = {\n",
    "    \"R18 FP32\":    \"#1f77b4\",\n",
    "    \"R18 INT8\":    \"#9467bd\",\n",
    "    \"R50 FP32\":    \"#1f77b4\",\n",
    "    \"R50 INT8\":    \"#9467bd\",\n",
    "    \"Ens FP32\":    \"#2ca02c\",\n",
    "    \"Ens INT8\":    \"#d62728\",\n",
    "    \"Ens Full\":    \"#7f7f7f\",\n",
    "}\n",
    "bar_colors = [colors[l] for l in labels]\n",
    "\n",
    "# ---------- (a) Test accuracy ----------\n",
    "ax = axes[0, 0]\n",
    "ax.bar(x, acc, width=bar_width, color=bar_colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=20, ha=\"right\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Test Accuracy\")\n",
    "ax.set_ylim(acc.min() - 0.01, acc.max() + 0.01)\n",
    "for i, v in enumerate(acc):\n",
    "    ax.text(i, v + 0.001, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# ---------- (b) Obstructed recall ----------\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x, rec_obst, width=bar_width, color=bar_colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=20, ha=\"right\")\n",
    "ax.set_ylabel(\"Recall (obstructed)\")\n",
    "ax.set_title(\"Per-class Recall – Obstructed (Class 1)\")\n",
    "ax.set_ylim(rec_obst.min() - 0.01, rec_obst.max() + 0.01)\n",
    "for i, v in enumerate(rec_obst):\n",
    "    ax.text(i, v + 0.001, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# ---------- (c) Single-image FPS (CPU) ----------\n",
    "ax = axes[1, 0]\n",
    "ax.bar(x, fps_single, width=bar_width, color=bar_colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=20, ha=\"right\")\n",
    "ax.set_ylabel(\"FPS (single image, CPU)\")\n",
    "ax.set_title(\"Single-image Inference Speed (CPU)\")\n",
    "ax.set_ylim(0, fps_single.max() * 1.15)\n",
    "for i, v in enumerate(fps_single):\n",
    "    ax.text(i, v + fps_single.max()*0.02, f\"{v:.1f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "# ---------- (d) Batch throughput (CPU) ----------\n",
    "ax = axes[1, 1]\n",
    "ax.bar(x, throughput_batch, width=bar_width, color=bar_colors)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=20, ha=\"right\")\n",
    "ax.set_ylabel(\"images / second (batch=64, CPU)\")\n",
    "ax.set_title(\"Batch Throughput (CPU, 64 images)\")\n",
    "ax.set_ylim(0, throughput_batch.max() * 1.15)\n",
    "for i, v in enumerate(throughput_batch):\n",
    "    ax.text(i, v + throughput_batch.max()*0.02, f\"{v:.1f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Hard-coded metrics\n",
    "# -----------------------------\n",
    "# Baseline: single ResNet50 FP32 (CPU)\n",
    "acc_r50_cpu        = 0.9180\n",
    "rec_obst_r50_cpu   = 0.9205\n",
    "fps_single_r50_cpu = 50.1\n",
    "thr_batch_r50_cpu  = 58.8\n",
    "\n",
    "# Hero: Quantized ResNet Ensemble (R18 + R50, FX INT8, CPU)\n",
    "acc_ens_q        = 0.9270\n",
    "rec_obst_ens_q   = 0.9245\n",
    "fps_single_ens_q = 80.7\n",
    "thr_batch_ens_q  = 275.6\n",
    "\n",
    "metrics = [\n",
    "    \"Accuracy\",\n",
    "    \"Obstructed Recall\",\n",
    "    \"Single FPS (CPU)\",\n",
    "    \"Batch Throughput (CPU)\",\n",
    "]\n",
    "\n",
    "baseline_vals = np.array([\n",
    "    acc_r50_cpu,\n",
    "    rec_obst_r50_cpu,\n",
    "    fps_single_r50_cpu,\n",
    "    thr_batch_r50_cpu,\n",
    "])\n",
    "\n",
    "ensemble_vals = np.array([\n",
    "    acc_ens_q,\n",
    "    rec_obst_ens_q,\n",
    "    fps_single_ens_q,\n",
    "    thr_batch_ens_q,\n",
    "])\n",
    "\n",
    "ratios = ensemble_vals / baseline_vals   # normalized to R50 FP32 = 1.0\n",
    "pct_deltas = (ratios - 1.0) * 100.0      # percentage change\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "y = np.arange(len(metrics))\n",
    "\n",
    "ax.barh(y, ratios, color=\"purple\", alpha=0.7)\n",
    "ax.axvline(1.0, color=\"gray\", linestyle=\"--\", linewidth=1.0)\n",
    "\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(metrics)\n",
    "ax.set_xlabel(\"Normalized to ResNet50 FP32 (CPU)  [Baseline = 1.0]\")\n",
    "ax.set_title(\n",
    "    \"Quantized ResNet Ensemble (R18 + R50, FX INT8, CPU)\\n\"\n",
    "    \"vs Single ResNet50 FP32 (CPU)\"\n",
    ")\n",
    "\n",
    "# Annotate each bar with ratio + % change\n",
    "for i, (r, d) in enumerate(zip(ratios, pct_deltas)):\n",
    "    ax.text(\n",
    "        r + 0.02, i,\n",
    "        f\"{r:.2f}  ({d:+.1f}%)\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "ax.set_xlim(0.9, max(1.1, ratios.max() + 1.0))\n",
    "ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-efficiency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
