{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8a45b6",
   "metadata": {},
   "source": [
    "Project Demo Working File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eff4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# gives you a separate window on desktop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2355d1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video meta: {'plugin': 'ffmpeg', 'nframes': inf, 'ffmpeg_version': '7.1-essentials_build-www.gyan.dev built with gcc 14.2.0 (Rev1, Built by MSYS2 project)', 'codec': 'h264', 'pix_fmt': 'yuv420p(tv, bt709, progressive)', 'audio_codec': 'aac', 'fps': 29.97, 'source_size': (640, 360), 'size': (640, 360), 'rotate': 0, 'duration': 36.01}\n",
      "Video FPS: 29.97\n",
      "Extracted frames: 1079\n",
      "Example frame: C:\\Users\\Andre\\Documents\\Machine Learning Project\\rear_end_2_frames\\frame_00000.png\n"
     ]
    }
   ],
   "source": [
    "# === Extract frames from rear_end_1.mp4 using imageio + ffmpeg ===\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from pathlib import Path\n",
    "from PIL import Image  # use PIL directly, simpler\n",
    "\n",
    "VIDEO_PATH = Path(r\"C:\\Users\\Andre\\Documents\\Machine Learning Project\\rear_end_2.mp4\")\n",
    "FRAMES_DIR = VIDEO_PATH.with_name(VIDEO_PATH.stem + \"_frames\")\n",
    "FRAMES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "reader = imageio.get_reader(str(VIDEO_PATH), format=\"ffmpeg\")\n",
    "meta = reader.get_meta_data()\n",
    "fps = meta.get(\"fps\", None)\n",
    "print(\"Video meta:\", meta)\n",
    "print(\"Video FPS:\", fps)\n",
    "\n",
    "frame_paths = []\n",
    "\n",
    "for idx, frame in enumerate(reader):\n",
    "    # frame: [H, W, C] uint8\n",
    "    pil_img = Image.fromarray(frame)  # no transpose\n",
    "\n",
    "    out_path = FRAMES_DIR / f\"frame_{idx:05d}.png\"\n",
    "    pil_img.save(out_path)\n",
    "    frame_paths.append(out_path)\n",
    "\n",
    "reader.close()\n",
    "\n",
    "print(\"Extracted frames:\", len(frame_paths))\n",
    "if frame_paths:\n",
    "    print(\"Example frame:\", frame_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64e6d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of frames: 1079\n",
      "Example frame path: C:\\Users\\Andre\\Documents\\Machine Learning Project\\rear_end_2_frames\\frame_00000.png\n"
     ]
    }
   ],
   "source": [
    "# === Frame Dataset & DataLoader for rear_end_1_frames ===\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Same normalization as training\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    ")\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, frames_dir, transform):\n",
    "        self.frames_dir = Path(frames_dir)\n",
    "        # Assuming our saved names are frame_00000.png, etc.\n",
    "        self.paths = sorted(self.frames_dir.glob(\"frame_*.png\"))\n",
    "        self.transform = transform\n",
    "\n",
    "        if not self.paths:\n",
    "            raise RuntimeError(f\"No frames found in {self.frames_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        return x, path.name  # return filename for later reference\n",
    "\n",
    "frames_ds = FrameDataset(FRAMES_DIR, eval_tf)\n",
    "frames_loader = DataLoader(\n",
    "    frames_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(f\"Number of frames: {len(frames_ds)}\")\n",
    "print(f\"Example frame path: {frames_ds.paths[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb39b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for models: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_29004\\3975301717.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FP32 ResNet18 + ResNet50 ensemble for demo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\AppData\\Local\\Temp\\ipykernel_29004\\3975301717.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(weights_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# === Load FP32 ResNet18 & ResNet50 for Demo Ensemble (CUDA if available) ===\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device for models:\", device)\n",
    "\n",
    "def build_resnet18_fp32(weights_path: str) -> nn.Module:\n",
    "    m = models.resnet18(weights=None)\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_feats, NUM_CLASSES)\n",
    "\n",
    "    state = torch.load(weights_path, map_location=\"cpu\")\n",
    "    m.load_state_dict(state)\n",
    "    m.to(device).eval()\n",
    "    return m\n",
    "\n",
    "def build_resnet50_fp32(weights_path: str) -> nn.Module:\n",
    "    m = models.resnet50(weights=None)\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_feats, NUM_CLASSES)\n",
    "\n",
    "    state = torch.load(weights_path, map_location=\"cpu\")\n",
    "    m.load_state_dict(state)\n",
    "    m.to(device).eval()\n",
    "    return m\n",
    "\n",
    "# Update these paths if your .pt files live elsewhere\n",
    "R18_WEIGHTS = r\"resnet18_clear_obstructed_best.pt\"\n",
    "R50_WEIGHTS = r\"resnet50_clear_obstructed_best.pt\"\n",
    "\n",
    "ens_r18 = build_resnet18_fp32(R18_WEIGHTS)\n",
    "ens_r50 = build_resnet50_fp32(R50_WEIGHTS)\n",
    "\n",
    "ensemble_models = [ens_r18, ens_r50]\n",
    "print(\"Loaded FP32 ResNet18 + ResNet50 ensemble for demo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e075f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran ensemble on 1079 frames.\n",
      "\n",
      "First 'obstructed' warning frame (>= 0.95):\n",
      "  Frame index: 612\n",
      "  Frame name:  frame_00612.png\n",
      "  Video time:  20.42 s\n",
      "frame 0607 (frame_00607.png): P(obstructed) = 0.944\n",
      "frame 0608 (frame_00608.png): P(obstructed) = 0.949\n",
      "frame 0609 (frame_00609.png): P(obstructed) = 0.948\n",
      "frame 0610 (frame_00610.png): P(obstructed) = 0.947\n",
      "frame 0611 (frame_00611.png): P(obstructed) = 0.943\n",
      "frame 0612 (frame_00612.png): P(obstructed) = 0.951\n",
      "frame 0613 (frame_00613.png): P(obstructed) = 0.941\n",
      "frame 0614 (frame_00614.png): P(obstructed) = 0.949\n",
      "frame 0615 (frame_00615.png): P(obstructed) = 0.954\n",
      "frame 0616 (frame_00616.png): P(obstructed) = 0.950\n"
     ]
    }
   ],
   "source": [
    "# === Run Ensemble Over Frames & Find First \"Obstructed\" Moment ===\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "all_probs = []     # P(obstructed) per frame\n",
    "all_names = []     # frame_000xx.png\n",
    "\n",
    "ensemble_models = [ens_r18, ens_r50]\n",
    "\n",
    "for xb, names in frames_loader:\n",
    "    xb = xb.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_sum = None\n",
    "        for m in ensemble_models:\n",
    "            out = m(xb)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "\n",
    "        logits = logits_sum / len(ensemble_models)\n",
    "        probs = F.softmax(logits, dim=1)[:, 1]  # class index 1 = \"obstructed\"\n",
    "\n",
    "    all_probs.extend(probs.cpu().tolist())\n",
    "    all_names.extend(list(names))\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "print(f\"Ran ensemble on {len(all_probs)} frames.\")\n",
    "\n",
    "# Choose a decision threshold for \"we would warn the driver\"\n",
    "OBSTRUCTED_THRESH = 0.95  # you can tune this later\n",
    "\n",
    "if np.any(all_probs >= OBSTRUCTED_THRESH):\n",
    "    trigger_idx = int(np.argmax(all_probs >= OBSTRUCTED_THRESH))\n",
    "    trigger_frame = all_names[trigger_idx]\n",
    "    trigger_time = trigger_idx / fps  # seconds, using fps from your earlier cell\n",
    "\n",
    "    print(f\"\\nFirst 'obstructed' warning frame (>= {OBSTRUCTED_THRESH:.2f}):\")\n",
    "    print(f\"  Frame index: {trigger_idx}\")\n",
    "    print(f\"  Frame name:  {trigger_frame}\")\n",
    "    print(f\"  Video time:  {trigger_time:.2f} s\")\n",
    "\n",
    "else:\n",
    "    trigger_idx = None\n",
    "    print(f\"\\nWARNING: P(obstructed) never crosses {OBSTRUCTED_THRESH:.2f} on this video.\")\n",
    "\n",
    "# Optional: peek at a few probabilities around the trigger\n",
    "for i in range(max(0, (trigger_idx or 0) - 5), min(len(all_probs), (trigger_idx or 0) + 5)):\n",
    "    print(f\"frame {i:04d} ({all_names[i]}): P(obstructed) = {all_probs[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83d421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1079 frames in C:\\Users\\Andre\\Documents\\Machine Learning Project\\rear_end_2_frames\n"
     ]
    }
   ],
   "source": [
    "# === Plot P(obstructed) vs frame/time for the FP32 ResNet ensemble ===\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Collect frame paths\n",
    "frame_paths = sorted(FRAMES_DIR.glob(\"frame_*.png\"))\n",
    "num_frames = len(frame_paths)\n",
    "print(f\"Found {num_frames} frames in {FRAMES_DIR}\")\n",
    "\n",
    "probs_obstructed = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for fp in frame_paths:\n",
    "        img = Image.open(fp).convert(\"RGB\")\n",
    "        x = eval_tf(img).unsqueeze(0).to(device)   # [1, 3, 224, 224]\n",
    "\n",
    "        # Ensemble logits (mean of models)\n",
    "        logits_sum = None\n",
    "        for m in ensemble_models:\n",
    "            out = m(x)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "        logits = logits_sum / len(ensemble_models)\n",
    "\n",
    "        # Class 1 = \"obstructed\"\n",
    "        p_obst = torch.softmax(logits, dim=1)[0, 1].item()\n",
    "        probs_obstructed.append(p_obst)\n",
    "\n",
    "probs_obstructed = np.array(probs_obstructed)\n",
    "\n",
    "# x-axis: time (seconds) if fps is known, else frame index\n",
    "frames = np.arange(num_frames)\n",
    "if fps is not None:\n",
    "    x_vals = frames / fps\n",
    "    x_label = \"Time [s]\"\n",
    "else:\n",
    "    x_vals = frames\n",
    "    x_label = \"Frame index\"\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x_vals, probs_obstructed, linewidth=1.5)\n",
    "plt.axhline(0.5, linestyle=\"--\", color=\"red\", label=\"0.5 threshold\")\n",
    "plt.xlabel(x_label)\n",
    "plt.ylabel(\"P(obstructed)\")\n",
    "plt.title(\"ResNet18+50 FP32 Ensemble: P(obstructed) over video\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff160404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1079 frames in C:\\Users\\Andre\\Documents\\Machine Learning Project\\rear_end_2_frames\n",
      "Video FPS (metadata): 29.97\n",
      "\n",
      "*** WARNING: model would alert at frame 615 (t=20.52s) ***\n",
      "Playback paused at warning frame. You can close the window when done.\n"
     ]
    }
   ],
   "source": [
    "# === \"Live\" playback with auto-pause on obstruction ===\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use the FP32 ResNet ensemble\n",
    "ensemble_models = [ens_r18, ens_r50]\n",
    "\n",
    "# Threshold & smoothing\n",
    "OBSTRUCTED_THRESH = 0.95      # tweak this (e.g., 0.90, 0.92, 0.95)\n",
    "MIN_CONSEC_FRAMES = 10         # require N consecutive frames above threshold\n",
    "\n",
    "frame_paths = sorted(FRAMES_DIR.glob(\"frame_*.png\"))\n",
    "num_frames = len(frame_paths)\n",
    "print(f\"Found {num_frames} frames in {FRAMES_DIR}\")\n",
    "print(f\"Video FPS (metadata): {fps}\")\n",
    "\n",
    "# Figure setup\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "im_artist = None\n",
    "text_artist = None\n",
    "\n",
    "consec_count = 0\n",
    "trigger_frame_idx = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, fp in enumerate(frame_paths):\n",
    "        frame_start = time.perf_counter()\n",
    "\n",
    "        # Load frame\n",
    "        img = Image.open(fp).convert(\"RGB\")\n",
    "\n",
    "        # Model input\n",
    "        x = eval_tf(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # Ensemble logits (mean of both models)\n",
    "        logits_sum = None\n",
    "        for m in ensemble_models:\n",
    "            out = m(x)\n",
    "            logits_sum = out if logits_sum is None else (logits_sum + out)\n",
    "        logits = logits_sum / len(ensemble_models)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        p_obst = probs[0, 1].item()\n",
    "\n",
    "        # --- Update display ---\n",
    "        if im_artist is None:\n",
    "            im_artist = ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "            text_artist = ax.text(\n",
    "                0.02,\n",
    "                0.95,\n",
    "                f\"t={idx / fps:5.2f}s  P(obstructed)={p_obst:.2f}\",\n",
    "                transform=ax.transAxes,\n",
    "                color=\"yellow\",\n",
    "                fontsize=12,\n",
    "                bbox=dict(facecolor=\"black\", alpha=0.5),\n",
    "            )\n",
    "        else:\n",
    "            im_artist.set_data(img)\n",
    "            text_artist.set_text(\n",
    "                f\"t={idx / fps:5.2f}s  P(obstructed)={p_obst:.2f}\"\n",
    "            )\n",
    "\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        # --- auto-tuned pause for near-real-time playback ---\n",
    "        compute_dt = time.perf_counter() - frame_start      # seconds spent this loop\n",
    "        target_dt = 1.0 / fps                               # desired frame interval\n",
    "        delay = max(target_dt - compute_dt, 0.0)            # don't go negative\n",
    "        plt.pause(delay)\n",
    "\n",
    "        # --- Threshold logic with consecutive-frame smoothing ---\n",
    "        if p_obst >= OBSTRUCTED_THRESH:\n",
    "            consec_count += 1\n",
    "            if consec_count >= MIN_CONSEC_FRAMES:\n",
    "                trigger_frame_idx = idx - MIN_CONSEC_FRAMES + 1\n",
    "                trigger_time = trigger_frame_idx / fps\n",
    "                print(\n",
    "                    f\"\\n*** WARNING: model would alert at \"\n",
    "                    f\"frame {trigger_frame_idx} (t={trigger_time:.2f}s) ***\"\n",
    "                )\n",
    "                break\n",
    "        else:\n",
    "            consec_count = 0\n",
    "\n",
    "if trigger_frame_idx is None:\n",
    "    print(\"\\nNo obstruction warning triggered with current threshold settings.\")\n",
    "else:\n",
    "    print(\"Playback paused at warning frame. You can close the window when done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-efficiency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
